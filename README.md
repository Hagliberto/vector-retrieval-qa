# Vector RetrievalQA com FAISS e LangChain

Esse projeto demonstra como criar um **sistema de recuperaÃ§Ã£o de informaÃ§Ãµes** baseado em vetores de embeddings, usando **FAISS** e **LangChain**, e responder perguntas em linguagem natural via **RetrievalQA**.

---

## ğŸ“‹ DescriÃ§Ã£o

1. **IngestÃ£o**  
   - Carrega documentos (PDF, TXT, MD).  
   - Divide em chunks de tamanho configurÃ¡vel.  
   - Gera embeddings com HuggingFace (modelo `all-MiniLM-L6-v2`).  
   - Indexa esses embeddings em um banco vetorial FAISS.

2. **Consulta (RetrievalQA)**  
   - Carrega o Ã­ndice FAISS previamente salvo.  
   - Consulta os chunks mais relevantes via embeddings da pergunta.  
   - Usa um LLM (Ollama Llama 3.2) para sintetizar a resposta.

---

## ğŸš€ Como executar

### 1. PrÃ©-requisitos

- Python 3.8+  
- Conta e setup do Ollama (se for usar OllamaLLM)  
- VariÃ¡vel de ambiente (opcional):  
  ```bash
  export OPENAI_API_KEY="sua_chave_aqui"
  ```

### 2. Setup

1. Clone este repositÃ³rio e entre na pasta:  
   ```bash
   git clone https://github.com/SEU_USUARIO/vector-retrieval-qa.git
   cd vector-retrieval-qa
   ```

2. Crie e ative o virtualenv:  
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # macOS/Linux
   source venv/bin/activate
   ```

3. Instale as dependÃªncias:  
   ```bash
   pip install -r requirements.txt
   ```

### 3. Gerar o Ã­ndice vetorial

Para **PDF**:  
```bash
python ingest.py data/seu_documento.pdf
```

Para **TXT/MD**:  
```bash
python ingest.py data/seu_texto.txt
```

- **SaÃ­da esperada:**  
  ```
  Gerados 256 chunks.
  âœ… Ãndice FAISS salvo em 'faiss_index'
  ```

### 4. Rodar o RetrievalQA

```bash
python qa.py
```

- Digite suas perguntas:  
  ```
  ğŸ‘‰ Qual o nome do curso?
  ğŸ—’ï¸ Resposta: Superior de Tecnologia em Sistemas para Internet.
  ```

- Para sair, digite `sair` ou `exit`.

---

## ğŸ“ Estrutura de pastas

```
vector-retrieval-qa/
â”œâ”€â”€ data/                 # Documentos para indexaÃ§Ã£o (PDF, TXT, MD)
â”‚   â”œâ”€â”€ exemplo.pdf
â”‚   â””â”€â”€ exemplo.txt
â”œâ”€â”€ faiss_index/          # Ãndice gerado pelo ingest.py
â”œâ”€â”€ ingest.py             # IngestÃ£o: chunking + embeddings + FAISS
â”œâ”€â”€ qa.py                 # Consulta: RetrievalQA + LLM
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # (este arquivo)
```

---

## âš™ï¸ ConfiguraÃ§Ãµes e personalizaÃ§Ãµes

- **Ajuste de chunking**:  
  No `ingest.py`, modifique:
  ```python
  chunk_size=1000
  chunk_overlap=50
  ```
- **Modelos de embeddings**:  
  - `all-MiniLM-L6-v2` (leve e rÃ¡pido)  
  - Outros: `sentence-transformers/all-mpnet-base-v2`, etc.
- **LLM**:  
  - Ollama Llama3.2 (`llama3.2:latest`)  


---

## ğŸ¥ DemonstraÃ§Ã£o em vÃ­deo

  1. ExecuÃ§Ã£o de `ingest.py` e criaÃ§Ã£o do Ã­ndice.  
  2. ExecuÃ§Ã£o de `qa.py` com perguntas e respostas.  
  3. Breve tour pelo README e estrutura de pastas.

ApresentaÃ§Ã£o:  
```
Parte 1: https://www.loom.com/share/43bcf623b7c84edcbf0df6bb55a7d2c5?sid=ae8664f8-efdd-407d-b8cf-6cfcb6b758b9

0:00
IntroduÃ§Ã£o ao Projeto
0:40
ConfiguraÃ§Ã£o do Arquivo de IngestÃ£o
1:28
Processamento e IndexaÃ§Ã£o
2:36
ExecuÃ§Ã£o do Arquivo de Consulta

Parte 2: https://www.loom.com/share/df280efff8fe41a680037985d491879a?sid=f62e86e5-5b5e-4311-a5be-9102703341c9
0:00
InteraÃ§Ã£o com o UsuÃ¡rio

```

---

## ğŸ“„ LicenÃ§a

MIT License Â© Hagliberto
